//////////   Step 1   //////////
//Using the geometry inputs in the map frame, 
//Please specify a rectangular geometry

//////////   Step 2   //////////
//Please specify a date below in the format YYYY, MM, DD
//Be sure to include parentheses around your numbers, e.g. '11'
//And also include '0' in front of numbers with only a single 
//digit, e.g. '04'
//If this date does not coincide with an image acqusition, 
//Search the first print message in the console 
//("Potential S1 Images)" and select the closest available 
//date from that list with dual polarization
var year = ('2019')
var month = ('01')
var day = ('03')


// If access is not granted to the above, you can download the data from the source
//link and upload it into your own repo. The script will work with line 15
//modified to your own asset repo. 
//
//Note that regardless of the input band types, they will be renamed VV and VH
//though this method is accurate for both polarizations. Please be aware
// of this when using the outputs!! 
//
//If the image is not dual polarized it will be rejected as these are
// unreliable for this methodology. 
//That's it! Happy coastal ice mapping. 


////////////////////////////////////////////////////////////////////////////////
///////////////////////////////////////////////////////////////////////////////
/////                                                                     /////
/////                   ***End of Inputs***                              /////
/////                                                                     /////
///////////////////////////////////////////////////////////////////////////////
///////////////////////////////////////////////////////////////////////////////

Map.centerObject(geometry)

var yearMonth = (year+'-'+month)
var doy = (yearMonth+'-'+day)
var date = ee.Algorithms.Date(doy)
var gdoy = ('g'+year+month+day)
var datedash = (year+month+day)

//Validation data table that must be specified.
//Data uploaded from source: https://www.glerl.noaa.gov/data/ice/#historical
var table3 = ee.FeatureCollection("users/marcacciojv/GLIceShapefiles/"+datedash)
//Must also rename column, which follows format of 'gYYYYMMDD', e.g. g20190105
/*
var table3 = table3.map(function(feat){
  return ee.Feature(feat.geometry(), { 
    iceconc: feat.get(gdoy),
    id: feat.get('Feature Index')
  })
})
*/
//Changing the data to no-ice, low-ice, and medium-ice, high-ice, full-ice
var table3 = table3.remap([0,10,20,30,40,50,60,70,80,90,95,100],[0,0,0,40,40,40,70,70,70,100,100,100], "iceconc")


// Function to perform angle correction
var angleCor = function toGamma0(image) {
 var hv = image.select('VH').subtract(image.select('angle')
 .multiply(Math.PI/180.0).cos().log10().multiply(10.0));
 return hv.addBands(image.select('VV').subtract(image.select('angle')
 .multiply(Math.PI/180.0).cos().log10().multiply(10.0)));
}
//Sentinel-1 Image Collection
var s1 = ee.ImageCollection("COPERNICUS/S1_GRD")


//Expand date search to Sentinel-1 image acqusition time, i.e. 10 days
var dateStart = ee.Date(date).advance(-5, 'day')
var dateEnd = ee.Date(date).advance(5, 'day')
//Print a list of Sentinel-1 images closest to your chosen date
var potential = s1.filterBounds(geometry).filterDate(dateStart, dateEnd)
print('Potential S1 Images', potential.filterDate(dateStart, dateEnd))

//One of two variable that must be specified. Choose the same date as the input table below
var s3march16 = s1.filterBounds(geometry).filterDate(date, date.advance(1, 'day')).map(angleCor)
print('Selected S1 Image', s3march16)
Map.addLayer(s3march16)

//var s1ce = s3march16.toBands()
var s1ce1 = s3march16.toBands().rename(['VV', 'VH'])
print('Checking that there is an image...', s1ce1)


var seeds = ee.Algorithms.Image.Segmentation.seedGrid(9);

var snic = ee.Algorithms.Image.Segmentation.SNIC({
  image: s1ce1,
  compactness:2,
  connectivity:8,
  seeds:seeds
});

var clusters = snic.select('clusters');
Map.addLayer(clusters.randomVisualizer(), {}, 'clusters', false)
//print(clusters);
var stdDev = s1ce1.addBands(clusters).reduceConnectedComponents(ee.Reducer.stdDev(), 'clusters', 256);
Map.addLayer(stdDev, {min:0, max:0.1}, 'StdDev', false);
//print(stdDev);
var area = ee.Image.pixelArea().addBands(clusters).reduceConnectedComponents(ee.Reducer.sum(), 'clusters', 256);
Map.addLayer(area, {min:10, max: 500000}, 'Cluster Area', false);
//print(area);


var objectPropertiesImage = ee.Image.cat([
  snic/*.select(bands)*/,
  stdDev,
  area,
]).float();
//New addition to create image out of feature collection
//var table3img = table3.reduceToImage(['iceconc'], ee.Reducer.firstNonNull)//.toInt()
//now stratified sampling so no class has over 100 points
//var table3strat = table3img.stratifiedSample({numPoints: 100, classBand: 'iceconc', region: s1ce1.geometry, scale: 20})

//a different way:
// Creating Class Image
var classes = ee.Image().byte().paint(table3, "iceconc").rename("iceconc").toUint8();
//Thought maybe this would work better since we are combining with a float later
//var classes = ee.Image().float().paint(table3, "iceconc").rename("iceconc").toUint8();
// concatenate with objectPropertiesImage
var classesImage = ee.Image.cat([objectPropertiesImage, classes])

// Sampling from Class Image - Returns FeatureCollections

var stratified = classesImage.addBands(ee.Image.pixelLonLat())
    .stratifiedSample({
      numPoints: 100, // This will be overridden by ClassValues & ClassPoints 
      classBand: 'iceconc',
      projection: 'EPSG:3857',//left is web mercator, right is NAD83 UTM 17N'EPSG:26917',
      scale: 20,
      region: s1ce1.geometry(),
      classValues: [0,40,70,100],//[0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 95, 100],
      classPoints: [750,400,400,400]//[200, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100]
    }).map(function(f) {
      return f.setGeometry(ee.Geometry.Point([f.get('longitude'), f.get('latitude')]));
    });

print(stratified);

//with image object properties
var trainingObj = objectPropertiesImage.sampleRegions({collection: table3, properties: ['iceconc'], scale: 20});




// The randomColumn() method will add a column of uniform random
// numbers in a column named 'random' by default.
trainingObj = stratified.randomColumn();

//Splitting for training vs validation
var split = 0.7;  // Roughly 70% training, 30% testing.
var trainingObjr = trainingObj.filter(ee.Filter.lt('random', split));
//print(trainingObj.size());
var validationObj = trainingObj.filter(ee.Filter.gte('random', split));

//Applying the classifier
var classifierObj = ee.Classifier.smileRandomForest(100).train(trainingObjr, 'iceconc', ['VV_mean', 'VH_mean']);
var classifiedObj = objectPropertiesImage.classify(classifierObj);
Map.addLayer(classifiedObj, {min:0, max:40}, 'classification Objects')

Export.image.toDrive({
  image: classifiedObj,
  description: 'IceExport'+datedash,
  folder: 'Ice_RSTrain_Revis_3c',
  fileNamePrefix: 'Ice'+datedash,
  region: geometry,
  scale: 20
  
})

//Training accuracy: Error matrix and absolute number
var trainAccuracy = classifierObj.confusionMatrix();
print('Ice Resubstitution error matrix: ', trainAccuracy);
print('Ice Training overall accuracy: ', trainAccuracy.accuracy());


//Same for held back Validation Dataset
var validaterObj = ee.Classifier.smileRandomForest(100).train(validationObj, 'iceconc', ['VV_mean', 'VH_mean']);
var validatedObj = objectPropertiesImage.classify(validaterObj);
var validaterAccuracy = validaterObj.confusionMatrix();
print('Ice Validation error matrix: ', validaterAccuracy);
print('Ice Validation overall accuracy: ', validaterAccuracy.accuracy());

//Converting confusion matrix to feature collection for export:
var exportAccuracy = ee.Feature(null, {matrix: trainAccuracy.array()});
var exportValidation = ee.Feature(null, {matrix: validaterAccuracy.array()});

//Creating a file with just accuracy, producer accuracy, and consumer accuracy
var exportValidNums = ee.FeatureCollection([ee.Feature(null,{
  'overall accuracy': validaterAccuracy.accuracy(), 
  'producer accuracy': validaterAccuracy.producersAccuracy(), 
  'consumer accuracy': validaterAccuracy.consumersAccuracy()
})]);

Export.table.toDrive({
  collection: exportValidNums,
  description: 'IceNums'+datedash,
  folder: 'Ice_Acc_Revis4_3c',
  fileNamePrefix: 'IceValidNums'+datedash
});

Export.table.toDrive({
  collection: ee.FeatureCollection(exportAccuracy),
  description: 'IceAcc'+datedash,
  folder: 'Ice_Acc_Revis_3c',
  fileNamePrefix: 'IceAcc'+datedash
});

Export.table.toDrive({
  collection: ee.FeatureCollection(exportValidation),
  description: 'IceValid'+datedash,
  folder: 'Ice_Acc_Revis_3x',
  fileNamePrefix: 'IceValid'+datedash
});
